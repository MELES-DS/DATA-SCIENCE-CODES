{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "131edb16-21b4-4db5-b8fc-faf07d9d1d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\student\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\student\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\student\\anaconda3\\lib\\site-packages (from python-docx) (4.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "874e085a-1160-4150-8dd5-213aff170cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a subset of artificial intelligence (AI) that enables systems to automatically learn from data and improve their performance over time without being explicitly programmed. It involves the development of algorithms that can identify patterns, make predictions, and adapt to new information based on the data they are exposed to. Machine learning is typically divided into three main types: supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, algorithms are trained on labeled data, meaning the input data is paired with the correct output, allowing the system to learn the relationship between the two. In unsupervised learning, the system is provided with data without labels and must find patterns or groupings within it. Reinforcement learning, on the other hand, involves an agent that learns by interacting with an environment and receiving feedback through rewards or penalties. The applications of machine learning are vast, ranging from natural language processing and image recognition to predictive analytics and autonomous systems. With the explosion of big data and advancements in computing power, machine learning is transforming industries like healthcare, finance, marketing, and entertainment, enabling more personalized experiences, improved decision-making, and innovative solutions to complex problems. However, challenges such as data privacy concerns, algorithmic bias, and the need for transparency in machine learning models still remain and need to be addressed as the field continues to evolve.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from docx import Document\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Step 3: Download NLTK resources (only need to run this once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "\n",
    "# Load the .docx file (make sure to use the correct path)\n",
    "doc = Document(r'C:\\Users\\Student\\Downloads\\machne learning.docx')\n",
    "\n",
    "# Read and print the content of the document\n",
    "for paragraph in doc.paragraphs:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "670e63fa-57fb-429a-9397-62d475f86508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the document: 226\n",
      "Word frequency: Counter({'the': 13, 'and': 12, 'learning': 11, 'to': 8, 'data': 7, 'machine': 5, 'is': 5, 'of': 4, 'in': 4, 'with': 4, 'that': 3, 'on': 3, 'are': 3, 'systems': 2, 'learn': 2, 'from': 2, 'without': 2, 'it': 2, 'involves': 2, 'algorithms': 2, 'patterns': 2, 'supervised': 2, 'unsupervised': 2, 'reinforcement': 2, 'system': 2, 'or': 2, 'an': 2, 'as': 2, 'need': 2, 'a': 1, 'subset': 1, 'artificial': 1, 'intelligence': 1, 'ai': 1, 'enables': 1, 'automatically': 1, 'improve': 1, 'their': 1, 'performance': 1, 'over': 1, 'time': 1, 'being': 1, 'explicitly': 1, 'programmed': 1, 'development': 1, 'can': 1, 'identify': 1, 'make': 1, 'predictions': 1, 'adapt': 1, 'new': 1, 'information': 1, 'based': 1, 'they': 1, 'exposed': 1, 'typically': 1, 'divided': 1, 'into': 1, 'three': 1, 'main': 1, 'types': 1, 'trained': 1, 'labeled': 1, 'meaning': 1, 'input': 1, 'paired': 1, 'correct': 1, 'output': 1, 'allowing': 1, 'relationship': 1, 'between': 1, 'two': 1, 'provided': 1, 'labels': 1, 'must': 1, 'find': 1, 'groupings': 1, 'within': 1, 'other': 1, 'hand': 1, 'agent': 1, 'learns': 1, 'by': 1, 'interacting': 1, 'environment': 1, 'receiving': 1, 'feedback': 1, 'through': 1, 'rewards': 1, 'penalties': 1, 'applications': 1, 'vast': 1, 'ranging': 1, 'natural': 1, 'language': 1, 'processing': 1, 'image': 1, 'recognition': 1, 'predictive': 1, 'analytics': 1, 'autonomous': 1, 'explosion': 1, 'big': 1, 'advancements': 1, 'computing': 1, 'power': 1, 'transforming': 1, 'industries': 1, 'like': 1, 'healthcare': 1, 'finance': 1, 'marketing': 1, 'entertainment': 1, 'enabling': 1, 'more': 1, 'personalized': 1, 'experiences': 1, 'improved': 1, 'decision': 1, 'making': 1, 'innovative': 1, 'solutions': 1, 'complex': 1, 'problems': 1, 'however': 1, 'challenges': 1, 'such': 1, 'privacy': 1, 'concerns': 1, 'algorithmic': 1, 'bias': 1, 'for': 1, 'transparency': 1, 'models': 1, 'still': 1, 'remain': 1, 'be': 1, 'addressed': 1, 'field': 1, 'continues': 1, 'evolve': 1})\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Read the content of the document\n",
    "text = []\n",
    "for paragraph in doc.paragraphs:\n",
    "    text.append(paragraph.text)\n",
    "\n",
    "# Join all paragraphs into a single string\n",
    "full_text = ' '.join(text)\n",
    "\n",
    "# Step 5: Process the text to find the sum and frequency of all words\n",
    "def process_text(text):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Calculate total number of words\n",
    "    total_words = len(words)\n",
    "    \n",
    "    # Calculate frequency of each word\n",
    "    word_frequency = Counter(words)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Total number of words in the document: {total_words}\")\n",
    "    print(f\"Word frequency: {word_frequency}\")\n",
    "\n",
    "# Step 6: Process the text to display statistics\n",
    "process_text(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "573c9268-9aaa-4ce3-804f-66c1a5b74854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\student\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\student\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Number of words removed (stop words): 84\n",
      "Filtered words (after removing stop words): ['machine', 'learning', 'subset', 'artificial', 'intelligence', 'ai', 'enables', 'systems', 'automatically', 'learn', 'data', 'improve', 'performance', 'time', 'without', 'explicitly', 'programmed', 'involves', 'development', 'algorithms', 'identify', 'patterns', 'make', 'predictions', 'adapt', 'new', 'information', 'based', 'data', 'exposed', 'machine', 'learning', 'typically', 'divided', 'three', 'main', 'types', 'supervised', 'learning', 'unsupervised', 'learning', 'reinforcement', 'learning', 'supervised', 'learning', 'algorithms', 'trained', 'labeled', 'data', 'meaning', 'input', 'data', 'paired', 'correct', 'output', 'allowing', 'system', 'learn', 'relationship', 'two', 'unsupervised', 'learning', 'system', 'provided', 'data', 'without', 'labels', 'must', 'find', 'patterns', 'groupings', 'within', 'reinforcement', 'learning', 'hand', 'involves', 'agent', 'learns', 'interacting', 'environment', 'receiving', 'feedback', 'rewards', 'penalties', 'applications', 'machine', 'learning', 'vast', 'ranging', 'natural', 'language', 'processing', 'image', 'recognition', 'predictive', 'analytics', 'autonomous', 'systems', 'explosion', 'big', 'data', 'advancements', 'computing', 'power', 'machine', 'learning', 'transforming', 'industries', 'like', 'healthcare', 'finance', 'marketing', 'entertainment', 'enabling', 'personalized', 'experiences', 'improved', 'decision', 'making', 'innovative', 'solutions', 'complex', 'problems', 'however', 'challenges', 'data', 'privacy', 'concerns', 'algorithmic', 'bias', 'need', 'transparency', 'machine', 'learning', 'models', 'still', 'remain', 'need', 'addressed', 'field', 'continues', 'evolve']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "# Step 2: Import necessary libraries\n",
    "from docx import Document\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Step 3: Download NLTK resources (only need to run this once)\n",
    "nltk.download('stopwords')\n",
    "def process_text(text):\n",
    "    # Initialize stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Count total words before removal\n",
    "    total_words = len(words)\n",
    "    \n",
    "    # Filter out stop words\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Count removed words\n",
    "    removed_words_count = total_words - len(filtered_words)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Number of words removed (stop words): {removed_words_count}\")\n",
    "    print(f\"Filtered words (after removing stop words): {filtered_words}\")\n",
    "\n",
    "# Step 7: Process the text to display statistics\n",
    "process_text(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa8ab06-3e23-4e61-a322-6a98081366c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\student\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\student\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\student\\anaconda3\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\student\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\student\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\student\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of unique root words: 107\n",
      "Unique root words: {'unsupervis', 'two', 'supervis', 'develop', 'market', 'big', 'ne', 'automatical', 'time', 'transform', 'industry', 'evolve', 'finance', 'subset', 'howev', 'healthcare', 'applica', 'autonomou', 'must', 'solu', 'machine', 'rang', 'address', 'interact', 'involve', 'artificial', 'learn', 'predictive', 'complex', 'without', 'typical', 'identify', 'expos', 'model', 'feedback', 'type', 'like', 'predic', 'remain', 'explicit', 'pow', 'allow', 'programm', 'correct', 'reinforce', 'field', 'informa', 'algorithmic', 'mak', 'performance', 'language', 'agent', 'image', 'explosion', 'adapt', 'train', 'pattern', 'relationship', 'main', 'bia', 'new', 'algorithm', 'personaliz', 'continue', 'input', 'innovative', 'penalty', 'data', 'decision', 'pair', 'hand', 'label', 'make', 'three', 'comput', 'divid', 'natural', 'vast', 'privacy', 'enable', 'improv', 'recogni', 'experience', 'entertain', 'receiv', 'process', 'output', 'within', 'problem', 'mean', 'still', 'reward', 'provid', 'transparency', 'challenge', 'environ', 'bas', 'intelligence', 'find', 'analytic', 'enabl', 'concern', 'group', 'advance', 'ai', 'improve', 'system'}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install the required libraries\n",
    "!pip install python-docx\n",
    "!pip install nltk\n",
    "\n",
    "# Step 2: Import necessary libraries\n",
    "from docx import Document\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Step 3: Download NLTK resources (only need to run this once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Step 4: Load the .docx file (make sure to use the correct path)\n",
    "doc = Document(r'C:\\Users\\Student\\Downloads\\machne learning.docx')\n",
    "\n",
    "# Step 5: Read the content of the document\n",
    "text = []\n",
    "for paragraph in doc.paragraphs:\n",
    "    text.append(paragraph.text)\n",
    "\n",
    "# Join all paragraphs into a single string\n",
    "full_text = ' '.join(text)\n",
    "\n",
    "# Step 6: Function to remove specific suffixes\n",
    "def remove_suffixes(word):\n",
    "    suffixes = ['ing', 'ed', 's', 'es', 'ly', 'er', 'est', 'ness', 'ment', 'tion', 'ity', 'adj']\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "# Step 7: Process the text to find root words\n",
    "def process_text(text):\n",
    "    # Initialize lemmatizer and stop words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Filter out stop words and lemmatize the remaining words\n",
    "    root_words = [remove_suffixes(lemmatizer.lemmatize(word)) for word in words if word not in stop_words]\n",
    "    \n",
    "    # Calculate the sum of unique root words\n",
    "    unique_root_words = set(root_words)\n",
    "    sum_unique_root_words = len(unique_root_words)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Sum of unique root words: {sum_unique_root_words}\")\n",
    "    print(f\"Unique root words: {unique_root_words}\")\n",
    "\n",
    "# Step 8: Process the text to display statistics\n",
    "process_text(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3720135-eb6c-4197-aadc-40c01f421bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
